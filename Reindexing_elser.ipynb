{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re -Indexing Script for Sparse index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "from datetime import datetime\n",
    "from elasticsearch import helpers, exceptions, RequestError\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Elastic Search Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a global client connection to elastic search\n",
    "es_client = Elasticsearch(\n",
    "    \"https://es-endpoint:port\",\n",
    "    basic_auth=(\"username\", \"password\"),\n",
    "    verify_certs=False,\n",
    "    request_timeout=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Creating Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the files from specific folder\n",
    "def get_all_files(folder_name):\n",
    "    # Change the directory\n",
    "    os.chdir(folder_name)\n",
    "    # iterate through all file\n",
    "    file_path_list =[]\n",
    "    for file in os.listdir():\n",
    "        print(file)\n",
    "        file_path = f\"{folder_name}/{file}\"\n",
    "        file_path_list.append(file_path)\n",
    "    return file_path_list\n",
    "\n",
    "\n",
    "## create the index\n",
    "def create_index(index_name,mapping):\n",
    "    try:\n",
    "        es_client.indices.create(index=index_name,body = mapping)\n",
    "        print(f\"Index '{index_name}' created successfully.\")\n",
    "    except RequestError as e:\n",
    "        if e.error == 'resource_already_exists_exception':\n",
    "            print(f\"Index '{index_name}' already exists.\")\n",
    "        else:\n",
    "            print(f\"An error occurred while creating index '{index_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. ELSER Model Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_model_name    = \".elser_model_2_linux-x86_64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.  Create ELSER ingest pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ingest_pipeline(client):\n",
    "    client.ingest.put_pipeline(\n",
    "            id=\"elser-ingest-pipeline\",\n",
    "            description=\"Ingest pipeline for ELSER\",\n",
    "            processors=[\n",
    "            {\n",
    "            \"inference\": {\n",
    "                \"model_id\": es_model_name,\n",
    "                 \"input_output\": [ \n",
    "                    {\n",
    "                        \"input_field\": \"chunk_text\",\n",
    "                        \"output_field\": \"chunk_tokens\"\n",
    "                    }\n",
    "                    ]\n",
    "            }\n",
    "            }\n",
    "        ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ingest_pipeline(es_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Create ELSER index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Index mapping \n",
    "elser_index_mapping = {\n",
    "    \"settings\" :{\n",
    "    \"index\": {\"default_pipeline\": \"elser-ingest-pipeline\"},\n",
    "    \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"refresh_interval\": \"1m\",\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"possessive_english_stemmer\": {\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"language\": \"possessive_english\"\n",
    "                },\n",
    "                \"light_english_stemmer\": {\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"language\": \"light_english\"\n",
    "                },\n",
    "                \"english_stop\": {\n",
    "                    \"ignore_case\": \"true\",\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords\": [\"a\", \"about\", \"all\", \"also\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\",\n",
    "                                  \"be\", \"been\", \"but\", \"by\", \"can\", \"de\", \"did\", \"do\", \"does\", \"for\", \"from\",\n",
    "                                  \"had\", \"has\", \"have\", \"he\", \"her\", \"him\", \"his\", \"how\", \"if\", \"in\", \"into\",\n",
    "                                  \"is\", \"it\", \"its\", \"more\", \"my\", \"nbsp\", \"new\", \"no\", \"non\", \"not\", \"of\",\n",
    "                                  \"on\", \"one\", \"or\", \"other\", \"our\", \"she\", \"so\", \"some\", \"such\", \"than\",\n",
    "                                  \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\",\n",
    "                                  \"thus\", \"to\", \"up\", \"us\", \"use\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\",\n",
    "                                  \"which\", \"while\", \"why\", \"will\", \"with\", \"would\", \"you\", \"your\", \"yours\"]\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"text_en_no_stop\": {\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"possessive_english_stemmer\",\n",
    "                        \"light_english_stemmer\"\n",
    "                    ],\n",
    "                    \"tokenizer\": \"standard\"\n",
    "                },\n",
    "                \"text_en_stop\": {\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"possessive_english_stemmer\",\n",
    "                        \"english_stop\",\n",
    "                        \"light_english_stemmer\"\n",
    "                    ],\n",
    "                    \"tokenizer\": \"standard\"\n",
    "                },\n",
    "                \"whitespace_lowercase\": {\n",
    "                    \"tokenizer\": \"whitespace\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"normalizer\": {\n",
    "                \"keyword_lowercase\": {\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"body_content\": {\"type\": \"text\"},\n",
    "            \"content_heading\": {\"type\": \"text\"},\n",
    "            \"domains\": {\"type\": \"keyword\"},\n",
    "            \"headings\": {\"type\": \"text\"},\n",
    "            \"last_crawled_at\": {\"type\": \"date\"},\n",
    "            \"links\": {\"type\": \"keyword\"},\n",
    "            \"main_content\": { \"type\": \"text\"},\n",
    "            \"meta_description\": {\"type\": \"text\"},\n",
    "            \"meta_keywords\": {\"type\": \"keyword\"},\n",
    "            \"title\": {\"type\": \"text\"},\n",
    "            \"url\": {\"type\": \"keyword\"},\n",
    "            \"url_path\": {\"type\": \"keyword\"},\n",
    "            \"chunk_no\":{\"type\":\"text\"},\n",
    "            \"chunk_text\":{\"type\":\"text\"},\n",
    "            \"chunk_heading\":{\"type\":\"text\"},\n",
    "            \"chunk_tokens\": { \"type\": \"rank_features\"}\n",
    "                 }\n",
    "            }\n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"main-index-name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index(index_name,elser_index_mapping) ## CHANGE INDEX NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer for chunking\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "def length_function(text):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "# create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    length_function = length_function,\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Get all docs from Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_docs_es(index_name):\n",
    "    try:\n",
    "        search_query={\n",
    "            \"query\": {\n",
    "                \"match_all\": {}\n",
    "            }\n",
    "            }\n",
    "        query_nested_index = es_client.search(\n",
    "        index=index_name,\n",
    "        body=search_query,\n",
    "        scroll='5m',  # Set the scroll timeout (e.g., 5 minutes)\n",
    "        size=800 ## set value to get all documnets\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return {\"msg\": \"Error searching indexes\", \"error\": e}\n",
    "    \n",
    "    # Get relevant chunks and format\n",
    "    references_context1 = [(chunks[\"_source\"], chunks[\"_score\"]) for chunks in query_nested_index[\"hits\"][\"hits\"]]\n",
    "    return references_context1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9. Create a docs for ELSER index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_doc_elser(resp):\n",
    "      document_list =[]\n",
    "      try:\n",
    "            l=0\n",
    "            for (ind, score) in resp:\n",
    "                print(\"***********************\", l)\n",
    "                l += 1\n",
    "                id = ind.get('id')\n",
    "                body_content = ind.get('body_content','')\n",
    "                content_heading= ind.get('content_heading','')\n",
    "                domains = ind.get('domains','')\n",
    "                url = ind.get('url', None)\n",
    "                headings = ind.get('headings', '')\n",
    "                last_crawled_at = ind.get('last_crawled_at', '')\n",
    "                links = ind.get('links', '')\n",
    "                main_content = ind.get('main_content', '')\n",
    "                meta_description = ind.get('meta_description', '')\n",
    "                meta_keywords = ind.get('meta_keywords', '')\n",
    "                title = ind.get('title', '')\n",
    "                url_path = ind.get('url_path', '')\n",
    "                if main_content == '':\n",
    "                      main_content = body_content\n",
    "                if len(main_content) > 512:\n",
    "                    chunks = text_splitter.split_text(main_content)\n",
    "                    for i,chunk in enumerate(chunks):\n",
    "                                doc_json = {\n",
    "                                    \"id\": id,\n",
    "                                    \"title\":title,\n",
    "                                    \"headings\":headings,\n",
    "                                    \"body_content\":body_content,\n",
    "                                    \"content_heading\":content_heading,\n",
    "                                    \"domains\":domains,\n",
    "                                    \"url\":url,\n",
    "                                    \"last_crawled_at\":last_crawled_at,\n",
    "                                    \"links\":links,\n",
    "                                    \"main_content\":main_content,\n",
    "                                    \"meta_description\":meta_description,\n",
    "                                    \"meta_keywords\":meta_keywords,\n",
    "                                    \"url_path\":url_path,\n",
    "                                    \"chunk_no\": i,\n",
    "                                    \"chunk_heading\":content_heading,\n",
    "                                    \"chunk_text\": chunk,\n",
    "                                        }\n",
    "                                document_list.append(doc_json)\n",
    "                else:\n",
    "                    doc_json = {\n",
    "                                    \"id\": id,\n",
    "                                    \"title\":title,\n",
    "                                    \"headings\":headings,\n",
    "                                    \"body_content\":body_content,\n",
    "                                    \"content_heading\":content_heading,\n",
    "                                    \"domains\":domains,\n",
    "                                    \"url\":url,\n",
    "                                    \"last_crawled_at\":last_crawled_at,\n",
    "                                    \"links\":links,\n",
    "                                    \"main_content\":main_content,\n",
    "                                    \"meta_description\":meta_description,\n",
    "                                    \"meta_keywords\":meta_keywords,\n",
    "                                    \"url_path\":url_path,\n",
    "                                    \"chunk_no\": 0,\n",
    "                                    \"chunk_heading\":content_heading,\n",
    "                                    \"chunk_text\": main_content,\n",
    "                                }\n",
    "                    document_list.append(doc_json)\n",
    "            return document_list\n",
    "      except Exception as e:\n",
    "         print(e)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10. Getting Documnets form base_crawlled_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index_name = 'base-index-name'  ## Change the index_name\n",
    "references_context1 = get_all_docs_es(base_index_name)\n",
    "len(references_context1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list = create_index_doc_elser(references_context1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(document_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11. Bulk Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "documents = []\n",
    "for doc in document_list:\n",
    "    documents.append(\n",
    "        {\n",
    "            \"_index\": index_name, ## CHANGE INDEX NAME\n",
    "            \"_source\": doc,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(documents, num_chunks):\n",
    "    chunk_size = len(documents) // num_chunks\n",
    "    remainder = len(documents) % num_chunks\n",
    "\n",
    "    start = 0\n",
    "    for i in range(num_chunks):\n",
    "        chunk_end = start + chunk_size + (1 if i < remainder else 0)\n",
    "        yield documents[start:chunk_end]\n",
    "        start = chunk_end\n",
    "\n",
    "# Example usage\n",
    "total_docs = len(documents)\n",
    "num_chunks = 19\n",
    "\n",
    "start_time = time.time()\n",
    "for i, chunk in enumerate(chunk_documents(documents, num_chunks)):\n",
    "    #clear_output(wait=True)\n",
    "    print(f\"Chunk {i+1}: {len(chunk)} documents\")\n",
    "    try:\n",
    "        response =helpers.bulk(es_client, chunk)\n",
    "        print(response)\n",
    "        print(\"Done indexing documents into \",{index_name}, \"index!\",{len(chunk)}) ## CHANGE INDEX NAME\n",
    "    except Exception as e: \n",
    "        # Handle the exception\n",
    "        print(\"An error occurred:\", e)\n",
    "    \n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time =end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
